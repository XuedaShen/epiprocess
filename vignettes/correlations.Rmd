---
title: 4. Compute correlations between signals
description: Compute correlations over space and time between signals.
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{4. Compute correlations between signals}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

The `epitools` package provides some simple functionality for computing the 
correlations between two signals, over space or time. In this vignette, we'll
examine state-level COVID-19 case and death rates, smoothed using a 7-day
trailing average.

```{r, message = FALSE}
library(covidcast)
library(epitools)
library(dplyr)

start_day <- "2020-03-01"
end_day <- "2021-10-11"

x <- covidcast_signal(data_source = "jhu-csse", 
                      signal = "confirmed_incidence_prop",
                      start_day = start_day, 
                      end_day = end_day,
                      geo_type = "state")  %>%
  as.epi_signal(name = "covid19_cases") %>%
  select(value, geo_value, time_value, issue) %>%
  slide_by_geo(slide_fun = ~ Mean(.x$value), n = 7, col_name = "value") 

y <- covidcast_signal(data_source = "jhu-csse", 
                      signal = "deaths_incidence_prop",
                      start_day = start_day, 
                      end_day = end_day,
                      geo_type = "state")  %>%
  as.epi_signal(name = "covid19_deaths") %>%
  select(value, geo_value, time_value, issue) %>%
  slide_by_geo(slide_fun = ~ Mean(.x$value), n = 7, col_name = "value") 
```

## Correlations sliced by time

The `sliced_cor()` function offers two ways to calculate correlations. The first 
way is to "slice by time": this calculates, for each time value, the correlation 
between the signals over all geographic locations. This is obtained by setting 
`by = "time_value"`.

```{r, message = FALSE, warning = FALSE, fig.width = 7, fig.height = 4}
library(ggplot2)
theme_set(theme_bw())

z1 <- sliced_cor(x, y, by = "time_value")

ggplot(z1, aes(x = time_value, y = value)) + geom_line() +
  scale_x_date(minor_breaks = "month", date_labels = "%b %y") +
  labs(x = "Date", y = "Correlation")
```

The above plot addresses the question: "on any given day, are case and death
rates linearly associated, across the U.S. states?". We might be interested in
broadening this question, instead asking: "on any given day, do higher case
rates tend to associate with higher death rates?", removing the dependence on a
linear relationship. The latter can be addressed using Spearman correlation,
accomplished by setting `method = "spearman"` in the call to `sliced_cor()`.
Spearman correlation is highly robust and invariant to monotone transformations.

## Lagged correlations

We might also be interested in how case rates associate with death rates in the
*future*. Using the `dt_x` parameter in `sliced_cor()`, we can lag case rates
back any number of days we want, before calculating correlations.

Below, we set `dt_x = -10`. This means that `x` will be lagged by 10 days, so
that cases on June 1st will be correlated with deaths on June 11th. (It might 
help to think of it this way: deaths on a certain day will be correlated with 
cases with an offset of -10 days.)

```{r, message = FALSE, warning = FALSE, fig.width = 7, fig.height = 4}
z2 <- sliced_cor(x, y, by = "time_value", dt_x = -10)

z <- rbind(z1 %>% mutate(lag = 0), 
           z2 %>% mutate(lag = 10)) %>%
  mutate(lag = as.factor(lag))

ggplot(z, aes(x = time_value, y = value)) +
  geom_line(aes(color = lag)) +
  scale_x_date(minor_breaks = "month", date_labels = "%b %y") +
  labs(x = "Date", y = "Correlation", col = "Lag") 
```

We can see that, generally, lagging the case rates back by 10 days improves the
correlations, confirming case rates are better correlated with death rates 10
days from now.

## Correlations sliced by state

The second option we have is to "slice by location": this calculates, for each
geographic location, correlation between the time series of two signals. This
is obtained by setting `by = "geo_value"`. We'll again look at correlations
both for observations at the same time and for 10-day lagged case rates.

```{r, message = FALSE, warning = FALSE, fig.width = 7, fig.height = 4}
z1 <- sliced_cor(x, y, by = "geo_value")
z2 <- sliced_cor(x, y, by = "geo_value", dt_x = -10)

z <- rbind(z1 %>% mutate(lag = 0), 
           z2 %>% mutate(lag = 10)) %>%
  mutate(lag = as.factor(lag))

ggplot(z, aes(value)) +
  geom_density(aes(fill = lag, col = lag), alpha = 0.5) +
  labs(x = "Correlation", y = "Density", fill = "Lag", col = "Lag") 
```

We can again see that, generally speaking, lagging the case rates back by 10 
days improves the correlations.

## More systematic lag analysis

Next we perform a more systematic investigation of the correlations over a broad
range of lag values. 

```{r, message = FALSE, warning = FALSE, fig.width = 7, fig.height = 4}
library(purrr)
lags = 0:35

z <- map_dfr(lags, function(lag) {
  sliced_cor(x, y, dt_x = -lag, by = "geo_value") %>% 
    mutate(lag = lag) 
  })

z %>%
  group_by(lag) %>%
  summarize(mean = Mean(value)) %>%
  ggplot(aes(x = lag, y = mean)) + geom_line() + geom_point() +
  labs(x = "dt", y = "Mean correlation")
```

We can see that some pretty clear curvature here in the mean correlation between 
case and death rates (where the correlations come from slicing by location) as
a function of lag. The maximum occurs at a lag of somewhere around 17 days.